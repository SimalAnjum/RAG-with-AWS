# RAG-with-AWS
### Real-Time Document Q&A System Using Retrieval-Augmented Generation (RAG)

---

## Team Members
- Dua e Sameen (ds07138)
- Muhammad Tahir Ghazi (mg07593)
- Simal Anjum (sa07716)

---

## 1. Introduction

This project presents a real-time document question-answering (Q&A) system using a Retrieval-Augmented Generation (RAG) pipeline.  
The system retrieves relevant document chunks from legal contracts and generates precise answers using a Large Language Model (LLM).

While the original plan involved extensive AWS services, the final system prioritizes a local deployment for cost-efficiency, focusing on fast retrieval and high answer accuracy.

---

## 2. Dataset & Use Case

**Dataset Used**:
- CUAD (Contract Understanding Atticus Dataset) â€” 13,000 expert-annotated clauses across 510 contracts.

**Primary Use Case**:
- Legal document question answering (e.g., compliance checks, contract analysis).

---

## 3. System Architecture

**Main Components**:
- **Document Storage**: Local file storage (S3 plan skipped).
- **Text Extraction**: Local PDF/TXT processing (Textract plan skipped).
- **Preprocessing**: Chunking documents (2048 tokens, 100-token overlap).
- **Embeddings**: BAAI/bge-small-en-v1.5 via LlamaIndex.
- **Vector Database**: FAISS for fast semantic search.
- **Answer Generation**: Llama-3.3-70B-Instruct-Turbo via TogetherAI API.
- **Backend Deployment**: FastAPI application with a `/rag` endpoint for document upload and query answering.

---

## 4. Key Technologies

- **FAISS**: Dense retrieval via cosine similarity.
- **LlamaIndex**: Document indexing, embedding, and chunking.
- **TogetherAI**: LLM API for high-quality answer generation.
- **FastAPI**: Lightweight web framework for deploying the RAG service.

---

## 5. Performance Metrics

The system was evaluated on:
- **Exact Match (EM)**: Whether predicted answers exactly match ground truth.
- **F1 Score**: Balance between precision and recall.
- **Mean Reciprocal Rank (MRR)**: Ranking effectiveness in retrieval.

Results showed strong retrieval and generation performance on legal domain queries.

---

## 6. How to Run the Project Locally

1. Clone the repository:
    ```bash
    git clone https://github.com/SimalAnjum/RAG-with-AWS.git
    cd RAG-with-AWS
    ```

2. Install required libraries:
    ```bash
    pip install -r requirements.txt
    ```

3. Set TogetherAI API Key:
    ```python
    import together
    together.api_key = 'YOUR_TOGETHER_API_KEY'
    ```

4. Start FastAPI server:
    ```bash
    uvicorn backend.main:app --reload
    ```

5. Use `/rag` endpoint:
   - Upload a document (PDF/TXT).
   - Submit a query.
   - Get a generated answer based on retrieved document chunks.

---

## 7. Expected Output

- Extracted and chunked text stored in local FAISS index.
- Real-time retrieval of top-k relevant chunks.
- Natural language answers generated by LLM grounded in real contract data.

Example:
> **Question**: "Does the agreement include a limitation of liability clause?"  
> **Answer**: "Yes. Section 10.2 limits liability to direct damages only."

---

## 8. Future Enhancements

- Add web-based user interface.
- Scale to full AWS deployment (S3, Textract, SageMaker).
- Fine-tune legal-specific LLMs (e.g., SaulLM, LawLLM).
- Expand to other legal datasets (e.g., case law, statutes).

---

## 9. Notes

- Some AWS components like S3 storage, Textract OCR, and SageMaker embeddings were replaced with local implementations due to cost limitations.
- TogetherAI was used for both embeddings and generation to maintain strong performance within budget.
- The architecture remains modular for easy transition to AWS cloud in the future.

---

# ðŸš€ Thank You for Exploring Our Project!
